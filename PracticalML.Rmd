---
title: "Data Science/Practical ML exercise"
author: "Nikolay Dobrinov"
date: "Oct 18, 2017"
output:
  pdf_document: default
  html_document:
    keep_md: yes
---


## Executive Summary
"One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). The training and testing data is located here:

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
"

The data was pre-processed and four calssification methods were tested. Random forests performs best, with 99.54% accuracy on 100 trees. The most important 5 variables in the random forest model are roll.belt, yaw.belt, magnet.dumbbell.z, magnet.dumbbell.y, pitch.belt.

## Data pre-processing
The seed is fixed for the purposes of reproducibility. We split the training data 
into a train and test data sets to cross validate the models. The testing data set provided for this assignment 
is used as a validation set. 
```{r, results="hide", message=FALSE, warning = FALSE}
setwd("/Users/nikolaydobrinov/Documents/work/Courses/R/WorkDirectory/Course8_week4_coding_assignments")

library(dplyr)
library(caret)

set.seed(333) # use a seed for replicability

# load data
traintest <- read.csv("./data/pml-training.csv", na.strings = c("NA", ""))
validate <- read.csv("./data/pml-testing.csv", na.strings = c("NA", ""))

# split train-test
inTrain <- createDataPartition(y=traintest$classe, p=0.7, list=FALSE)
train <- traintest[inTrain,]
test <- traintest[-inTrain,]
```

Remove variables that do not seem useful, or it is not clear what they represent. 
Note that user_name should not be used in clasification as the prediction algorithm 
should work regardless of the specific user using the device Remove the first 7 columns
```{r}
train <- select(train, -(1:7))
test <- select(test, -(1:7))
validate <- select(validate, -(1:7))
```
Remove variables with NAs. naVars below reveals that in all variables where NAs exist, 
about 98% of the observations are NA. We remove all of these features
```{r}
naVars <- sapply(train, function(x) sum(is.na(x)))/nrow(train)
naVarsExclude <- names(naVars[naVars > 0]) 
train <- train[, !names(train) %in% naVarsExclude]
test <- test[, !names(test) %in% naVarsExclude] 
validate <- validate[, !names(validate) %in% naVarsExclude]  
```
Check for variables with low variation and remove them. There are no variables with low or zero variance
```{r}
lowVariance <- nearZeroVar(train, saveMetrics=TRUE)
sum(lowVariance$zeroVar) + sum(lowVariance$nzv)
```

## Analysis

I fit four classification methods - tree, random forest, generalized boosted 
regression (gbm), and linear discriminant analysis (lda). The corresponding 
classification accuracy of the models on the testing sampe is: single tree - 49%; 
random forest with 100 trees - 99.34%; gbm - 96%; lda - 71%. This section presents the results 
on the lowest error model, random forests, the rest of the models are presented in the Appendix.

The random forest model with 100 trees produces an expected error on the test sample of 0.66%. The variable importance function reveals that across the 100 trees the most important 5 variables are related to the belt and the position of the dumbbel: roll.belt, yaw.belt, magnet.dumbbell.z, magnet.dumbbell.y, pitch.belt. Optimal number of variables to be randomly sampled as candidates at each split is mtry=2.
```{r, results="hide", echo=FALSE, message=FALSE, warning = FALSE, cache=TRUE}
modFit.rf <- train(classe~ .,method="rf", ntree=100, data=train)
```

```{r}
pred.rf <- predict(modFit.rf,test) # predict on test data
confusionMatrix(pred.rf,test$classe) # measure accuracy on test data
varImp(modFit.rf) # variable importance
print(modFit.rf) # optimal mtry
predict(modFit.rf,validate) # predict on validation data
```

We already fixed the tuning parameter ntree=100, but we can try another search for the optimal tuning parameter mtry. Below we use a different resampling method - 10 fold cross-validation repeated 3 times. This setup runs much longer, because of the three repeats, and provides a marginal improvement to the accuracy. The expected error is reduced to about 0.46%. The most important variables are the same, however the optimal mtry tuning parameter changes to more than 2 variables. 
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
mtry <- sqrt(ncol(train))
```

```{r, results="hide", echo=FALSE, message=FALSE, warning = FALSE, cache=TRUE}
modFit.rf_random <- train(classe~., data=train, method="rf", metric="Accuracy", tuneLength=15, trControl=control, ntree=100)
```

```{r}
pred.rf_random <- predict(modFit.rf_random,test) # predict on test data
confusionMatrix(pred.rf_random,test$classe) # measure accuracy on test data
varImp(modFit.rf_random) # variable importance
print(modFit.rf_random) # optimal mtry
predict(modFit.rf_random,validate) # predict on validation data
```

## Appendix
The results from the lower performing models for this data set are presented below.

- Decision tree:
```{r, results="hide", echo=FALSE, message=FALSE, warning = FALSE, cache=TRUE}
modFit.tree <- train(classe~ .,method="rpart", data=train)
```
```{r}
pred.tree <- predict(modFit.tree,test); confusionMatrix(pred.tree,test$classe)
```

- Generalized boosted regression (gbm):
```{r, results="hide", echo=FALSE, message=FALSE, warning = FALSE, cache=TRUE}
modFit.gbm <- train(classe~ .,method="gbm", data=train)
```
```{r}
pred.gbm <- predict(modFit.gbm,test); confusionMatrix(pred.gbm,test$classe)
```

- Linear discrimination analysis:
```{r, results="hide", echo=FALSE, message=FALSE, warning = FALSE, cache=TRUE}
modFit.lda <- train(classe~ .,method="lda", data=train)
```
```{r}
pred.lda <- predict(modFit.lda,test); confusionMatrix(pred.lda,test$classe)
```

